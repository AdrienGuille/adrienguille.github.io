# Teaching

## Representation Learning for NLP (M2 Data Mining)

Here is a list of suggested readings for this course:

### Pre-Trained Word Representations

##### Main techniques to learn vector space representations of words

- SGNS: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- Link between SGNS, GloVe & PMI: [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)

### Supervised Learning with Pre-Trained Word Representations

##### Main neural architectures to solve document-level classification tasks 

- CNN: [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)
- RNN & GRU: [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)

##### Sequence to Sequence learning and attention mechanism

- Seq2Seq: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
- Dot-product Attention: [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
- Transformer: [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

  ### Supervised Learning with Pre-Trained Language Models

- BERT: [Bidirectional Encoder Representations from Transformers](https://arxiv.org/pdf/1810.04805.pdf)

## Data Mining (DUT2 Statistique et Informatique Décisionnelle)

Course material available on [Moodle](https://moodle.univ-lyon2.fr).

## Bases de la programmation (DUT1 Statistique et Informatique Décisionnelle)

Course material available on [Moodle](https://moodle.univ-lyon2.fr).
