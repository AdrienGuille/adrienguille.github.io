# Teaching

## Representation Learning for NLP (Master Machine Learning & AI @ U Lyon 2)

Course outline and suggested readings:

### Pre-LLM Era

#### Learning vector space representations of words

- SGNS: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- Link between SGNS, GloVe & PMI: [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)

#### Leveraging pre-trained word representations for supervised learning

##### Basic architectures for text classification 

- CNN: [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)
- RNN & GRU: [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)

##### Advanced architecture for text generation

- Seq2Seq: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
- Attention: [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)

### LLM Era

#### Foundation models 

- Transformer: [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- BERT: [Bidirectional Encoder Representations from Transformers](https://arxiv.org/pdf/1810.04805.pdf)
- GPT: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- T5: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)

#### Adapting foundation models

- RAG: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)
- LoRA: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)

## Machine Learning with Graphs (Master Complex Systems @ ENS Lyon)

Course outline and suggested readings:

### Pre-trained Node and Graph Representations

- DeepWalk: [Online Learning of Social Representations](https://arxiv.org/pdf/1403.6652)
- Node2Vec: [Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf)

### Graph Neural Networks

- GCN: [Semi Supervised Node Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf)
- GAT: [Graph Attention Networks](https://arxiv.org/pdf/1710.10903.pdf)

## Reinforcement Learning (Master Machine Learning & AI @ U Lyon 2)

Course outline and suggested readings:

### Aligning a LLM by reinforcement learning

- RLHF: [Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325)
- InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)

### Practical implementation
- PPO: [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
- DPO: [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
